{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_neg = []\n",
    "for filename in os.listdir('./data/train/neg'):\n",
    "    file = open('./data/train/neg/' + filename, 'r')\n",
    "    train_texts_neg.append(file.read())\n",
    "    \n",
    "train_texts_pos = []\n",
    "for filename in os.listdir('./data/train/pos'):\n",
    "    file = open('./data/train/pos/' + filename, 'r')\n",
    "    train_texts_pos.append(file.read())\n",
    "    \n",
    "test_texts_neg = []\n",
    "for filename in os.listdir('./data/test/neg'):\n",
    "    file = open('./data/test/neg/' + filename, 'r')\n",
    "    test_texts_neg.append(file.read())\n",
    "    \n",
    "test_texts_pos = []\n",
    "for filename in os.listdir('./data/test/pos'):\n",
    "    file = open('./data/test/pos/' + filename, 'r')\n",
    "    test_texts_pos.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample for the purpose of speed\n",
    "train_texts_neg = train_texts_neg[:100]\n",
    "train_texts_pos = train_texts_pos[:100]\n",
    "test_texts_neg = test_texts_neg[:100]\n",
    "test_texts_pos = test_texts_pos[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts_neg))\n",
    "print(len(train_texts_pos))\n",
    "print(len(test_texts_neg))\n",
    "print(len(test_texts_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_neg_tok = [tok(line) for line in train_texts_neg]\n",
    "train_texts_pos_tok = [tok(line) for line in train_texts_pos]\n",
    "test_texts_neg_tok = [tok(line) for line in test_texts_neg]\n",
    "test_texts_pos_tok = [tok(line) for line in test_texts_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_neg_stop = [[word for word in sent if not (word.is_stop or word.is_punct)] for sent in train_texts_neg_tok]\n",
    "train_texts_pos_stop = [[word for word in sent if not (word.is_stop or word.is_punct)] for sent in train_texts_pos_tok]\n",
    "test_texts_neg_stop = [[word for word in sent if not (word.is_stop or word.is_punct)] for sent in test_texts_neg_tok]\n",
    "test_texts_pos_stop = [[word for word in sent if not (word.is_stop or word.is_punct)] for sent in test_texts_pos_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should also remove all the .<br> and the like, but no point in it now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts_neg_lem = [[word.lemma_ for word in sent if word.has_vector and not (word.is_stop or word.is_punct)] for sent in train_texts_neg_stop]\n",
    "# train_texts_pos_lem = [[word.lemma_ for word in sent if word.has_vector and not (word.is_stop or word.is_punct)] for sent in train_texts_pos_stop]\n",
    "# test_texts_neg_lem = [[word.lemma_ for word in sent if word.has_vector and not (word.is_stop or word.is_punct)] for sent in test_texts_neg_stop]\n",
    "# test_texts_pos_lem = [[word.lemma_ for word in sent if word.has_vector and not (word.is_stop or word.is_punct)] for sent in test_texts_pos_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_neg_lem = train_texts_neg_stop\n",
    "train_texts_pos_lem = train_texts_pos_stop\n",
    "test_texts_neg_lem = test_texts_neg_stop\n",
    "test_texts_pos_lem = test_texts_pos_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_neg_vect = [[word for word in sent if word.has_vector] for sent in train_texts_neg_lem]\n",
    "train_texts_pos_vect = [[word for word in sent if word.has_vector] for sent in train_texts_pos_lem]\n",
    "test_texts_neg_vect = [[word for word in sent if word.has_vector] for sent in test_texts_neg_lem]\n",
    "test_texts_pos_vect = [[word for word in sent if word.has_vector] for sent in test_texts_pos_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words  = set(\n",
    "    [word for sentence in train_texts_neg_lem for word in sentence] + \\\n",
    "    [word for sentence in train_texts_pos_lem for word in sentence] + \\\n",
    "    [word for sentence in test_texts_neg_lem for word in sentence] + \\\n",
    "    [word for sentence in test_texts_pos_lem for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ix = {word: i for i, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_str = 'pad'\n",
    "pad_int = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ix[pad_str] = pad_int\n",
    "ix2word = {i: word for word, i in word2ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(\n",
    "[max([len(sent) for sent in train_texts_neg_vect])] + \n",
    "[max([len(sent) for sent in train_texts_pos_vect])] +\n",
    "[max([len(sent) for sent in test_texts_neg_vect])] + \n",
    "[max([len(sent) for sent in test_texts_pos_vect])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(sent, max_len):\n",
    "    return sent + [pad_str]*(max_len - len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_neg_pad = [pad_seq(sent, max_len) for sent in train_texts_neg_vect]\n",
    "train_texts_pos_pad = [pad_seq(sent, max_len) for sent in train_texts_pos_vect]\n",
    "test_texts_neg_pad = [pad_seq(sent, max_len) for sent in test_texts_neg_vect]\n",
    "test_texts_pos_pad = [pad_seq(sent, max_len) for sent in test_texts_pos_vect]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to Padded Integer Lists of Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_neg_int = [[word2ix[word] for word in sent] for sent in train_texts_neg_pad]\n",
    "train_texts_pos_int = [[word2ix[word] for word in sent] for sent in train_texts_pos_pad]\n",
    "test_texts_neg_int = [[word2ix[word] for word in sent] for sent in test_texts_neg_pad]\n",
    "test_texts_pos_int = [[word2ix[word] for word in sent] for sent in test_texts_pos_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_neg_tens = [torch.tensor(sent, dtype=torch.long) for sent in train_texts_neg_int]\n",
    "train_texts_pos_tens = [torch.tensor(sent, dtype=torch.long) for sent in train_texts_pos_int]\n",
    "test_texts_neg_tens = [torch.tensor(sent, dtype=torch.long) for sent in test_texts_neg_int]\n",
    "test_texts_pos_tens = [torch.tensor(sent, dtype=torch.long) for sent in test_texts_pos_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare weight matrix with Spacy's word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.zeros((len(all_words)+1, 384)) ## 384 is Spacy's builtin vector length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(all_words):\n",
    "    weights_matrix[i] = word.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will create the layer and return the proper sizes\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': torch.Tensor(weights_matrix)})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_texts_neg_tens + train_texts_pos_tens\n",
    "X_test = test_texts_neg_tens + test_texts_pos_tens\n",
    "y_train = [0] * len(train_texts_neg_pairs) + [1] * len(train_texts_pos_pairs)\n",
    "y_test = [0] * len(test_texts_neg_pairs) + [1] * len(test_texts_pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [torch.tensor(tag, dtype=torch.float) for tag in y_train]\n",
    "y_test = [torch.tensor(tag, dtype=torch.float) for tag in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBSentimentDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.X[idx], 'tag': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = IMDBSentimentDataset(X_train, y_train)\n",
    "X_test_dataset = IMDBSentimentDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(X_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(X_test_dataset, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin with a very simple model, just to test things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleRNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(simpleRNN, self).__init__()\n",
    "        self.size_hidden = 64\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix)\n",
    "        self.GRU = nn.GRU(embedding_dim, self.size_hidden, 1, batch_first=True)\n",
    "        self.lin = nn.Linear(self.size_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.size_hidden),\n",
    "                torch.zeros(1, 1, self.size_hidden))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        out, hidden = self.GRU(embeds)\n",
    "        hid = hidden.view((-1,self.size_hidden))\n",
    "        probs = self.sigmoid(self.lin(hid)).view(-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleRNN()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = np.array([])\n",
    "    labels = np.array([])\n",
    "    for batch in testloader:\n",
    "        preds = np.append(preds, model(batch['text']).numpy())\n",
    "        labels = np.append(labels, batch['tag'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.where(preds > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Accuracy is 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-Accuracy is {}\".format(np.mean(preds == labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now in epoch 0 in batch 0\n",
      "now in epoch 0 in batch 1\n",
      "now in epoch 0 in batch 2\n",
      "now in epoch 0 in batch 3\n",
      "now in epoch 0 in batch 4\n",
      "now in epoch 0 in batch 5\n",
      "now in epoch 0 in batch 6\n",
      "in epoch 0 the loss is 4.868297100067139\n",
      "now in epoch 1 in batch 0\n",
      "now in epoch 1 in batch 1\n",
      "now in epoch 1 in batch 2\n",
      "now in epoch 1 in batch 3\n",
      "now in epoch 1 in batch 4\n",
      "now in epoch 1 in batch 5\n",
      "now in epoch 1 in batch 6\n",
      "in epoch 1 the loss is 4.838747978210449\n",
      "now in epoch 2 in batch 0\n",
      "now in epoch 2 in batch 1\n",
      "now in epoch 2 in batch 2\n",
      "now in epoch 2 in batch 3\n",
      "now in epoch 2 in batch 4\n",
      "now in epoch 2 in batch 5\n",
      "now in epoch 2 in batch 6\n",
      "in epoch 2 the loss is 4.830099105834961\n",
      "now in epoch 3 in batch 0\n",
      "now in epoch 3 in batch 1\n",
      "now in epoch 3 in batch 2\n",
      "now in epoch 3 in batch 3\n",
      "now in epoch 3 in batch 4\n",
      "now in epoch 3 in batch 5\n",
      "now in epoch 3 in batch 6\n",
      "in epoch 3 the loss is 4.858771800994873\n",
      "now in epoch 4 in batch 0\n",
      "now in epoch 4 in batch 1\n",
      "now in epoch 4 in batch 2\n",
      "now in epoch 4 in batch 3\n",
      "now in epoch 4 in batch 4\n",
      "now in epoch 4 in batch 5\n",
      "now in epoch 4 in batch 6\n",
      "in epoch 4 the loss is 4.768999099731445\n"
     ]
    }
   ],
   "source": [
    "loss_acc =  0\n",
    "for epoch in range(5):\n",
    "    for i_batch, sample_batched in enumerate(trainloader):\n",
    "        print(\"now in epoch {} in batch {}\".format(epoch, i_batch))\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        texts = sample_batched['text']\n",
    "        tags = sample_batched['tag']\n",
    "        \n",
    "        preds = model(texts)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(preds, tags)\n",
    "        loss_acc += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"in epoch {} the loss is {}\".format(epoch, loss_acc))\n",
    "    loss_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = np.array([])\n",
    "    labels = np.array([])\n",
    "    for batch in testloader:\n",
    "        preds = np.append(preds, model(batch['text']).numpy())\n",
    "        labels = np.append(labels, batch['tag'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.where(preds > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Accuracy is 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Post-Accuracy is {}\".format(np.mean(preds == labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
